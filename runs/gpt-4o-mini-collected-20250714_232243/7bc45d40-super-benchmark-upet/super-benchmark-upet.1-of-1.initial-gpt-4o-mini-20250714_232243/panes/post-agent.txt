b5fc42e75c9db75b96da6053ad)
07/14/2025 23:39:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 950.44it/s]
07/14/2025 23:39:43 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f14f57d24d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:39:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:39:43 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f14f57d0670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:39:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:39:43 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f14f57d24d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:39:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:39:43,804 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:39:43,805 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:39:43,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:39:44,804 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cl
s.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:39:44,804 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:39:54 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:39:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-39-54_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:39:54,825 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:39:54,826 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:39:55,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:39:55,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:39:55,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:39:55,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:39:55,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:39:55,443 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:39:55,444 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:39:55 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:39:55 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:39:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:39:55 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:39:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 937.07it/s]
07/14/2025 23:39:55 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f214b5224d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:39:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:39:55 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f214b520670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:39:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:39:55 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f214b5224d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:39:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:39:55,815 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:39:55,816 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:39:55,906 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:39:56,816 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relations
hip.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:39:56,816 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:40:05 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:40:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-40-05_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:40:05,700 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:05,701 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:06,151 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:06,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:06,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:06,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:06,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:40:06,232 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:06,233 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:40:06 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:40:06 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:40:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:40:06 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:40:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 682.11it/s]
07/14/2025 23:40:06 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f6e73bd64d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:40:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:40:06 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f6e73bd4670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:40:06 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f6e73bd64d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:40:06,598 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:06,599 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:40:06,740 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:40:07,640 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight
', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:40:07,640 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:40:16 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:40:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-40-16_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:40:17,173 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:17,174 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:17,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:17,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:17,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:17,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:17,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:40:17,695 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:17,695 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:40:17 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:40:17 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:40:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:40:17 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:40:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 942.54it/s]
07/14/2025 23:40:17 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7fdffa3224d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:40:17 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:40:17 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7fdffa320670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:17 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:40:17 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7fdffa3224d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:17 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:40:18,032 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:18,033 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:40:18,127 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:40:19,024 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relation
ship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:40:19,024 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:40:27 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:40:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-40-27_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:40:28,417 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:28,418 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:28,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:28,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:28,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:28,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:28,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:40:28,947 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:28,948 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:40:29 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:40:29 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:40:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:40:29 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:40:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 985.58it/s]
07/14/2025 23:40:29 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7fd5033e24d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:40:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:40:29 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7fd5033e0670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:40:29 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7fd5033e24d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:40:29,336 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:29,337 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:40:29,428 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:40:30,351 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight
', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:40:30,351 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:40:39 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:40:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-40-39_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:40:39,620 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:39,621 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:40,294 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:40,294 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:40,294 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:40,294 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:40,294 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:40:40,392 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:40,393 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:40:40 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:40:40 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:40:40 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:40:40 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:40:40 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 913.00it/s]
07/14/2025 23:40:40 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f8cbeb1a4d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:40:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:40:40 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f8cbeb18670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:40:40 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f8cbeb1a4d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:40:40,781 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:40,782 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:40:40,894 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:40:41,805 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cl
s.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:40:41,805 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:40:56 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:40:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-40-56_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:40:56,583 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:56,584 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:57,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:57,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:57,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:57,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:40:57,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:40:57,338 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:57,338 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:40:57 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:40:57 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:40:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:40:57 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:40:57 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 885.87it/s]
07/14/2025 23:40:57 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7ff7631d64d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:40:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:40:57 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7ff7631d4670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:40:57 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7ff7631d64d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:40:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:40:57,723 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:40:57,724 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:40:57,841 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:40:58,766 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.
bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:40:58,766 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET# python run.py --model_name_or_path bert-base-uncased --task_name glue --dataset_name rte --seed 42 --output_dir /app/re
sults --report_to none; tmux wait -S done
07/14/2025 23:41:07 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
07/14/2025 23:41:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/app/results/runs/Jul14_23-41-07_0ea2a2d76b33,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=/app/results,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/app/results,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:674] 2025-07-14 23:41:07,983 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:41:07,984 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:41:08,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /ro
ot/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf
2c99
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:41:08,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache a
t /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc70
4371bdfa4
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:41:08,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cach
e at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:41:08,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json fro
m cache at None
[INFO|tokenization_utils_base.py:1803] 2025-07-14 23:41:08,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from
cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973
f6630a152e4b9aed
[INFO|configuration_utils.py:674] 2025-07-14 23:41:08,681 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:41:08,682 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

07/14/2025 23:41:08 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07
a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
07/14/2025 23:41:08 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/14/2025 23:41:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
07/14/2025 23:41:08 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfe
b5fc42e75c9db75b96da6053ad)
07/14/2025 23:41:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5
fc42e75c9db75b96da6053ad
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 886.43it/s]
07/14/2025 23:41:08 - WARNING - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f32637ee4d0> of the transform datase
ts.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with
 pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the
 previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
07/14/2025 23:41:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow
07/14/2025 23:41:08 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f32637ec670> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:41:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow
07/14/2025 23:41:08 - INFO - datasets.fingerprint - Parameter 'function'=<function GlueDataset.preprocess_function at 0x7f32637ee4d0> of the transform datasets.
arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
07/14/2025 23:41:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70
367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow
[INFO|configuration_utils.py:674] 2025-07-14 23:41:09,064 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from c
ache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52
b1a461abe3b9e4e
[INFO|configuration_utils.py:723] 2025-07-14 23:41:09,065 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "rte",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2034] 2025-07-14 23:41:09,193 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache
 at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54
a29633f7f2f
[WARNING|modeling_utils.py:2418] 2025-07-14 23:41:10,321 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSeq
uenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'c
ls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.
g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializin
g a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2430] 2025-07-14 23:41:10,321 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert
-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** total param is 109483778 *****
(venv) root@0ea2a2d76b33:/app/UPET/UPET# ls /app/results; tmux wait -S done
(venv) root@0ea2a2d76b33:/app/UPET/UPET# cat /app/results/results.json; tmux wait -S done
cat: /app/results/results.json: No such file or directory
(venv) root@0ea2a2d76b33:/app/UPET/UPET#
